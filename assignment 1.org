#+STARTUP: indent
#+OPTIONS: toc:nil num:nil
#+TITLE: ECON 497 Assignment 1
#+LaTeX_CLASS_OPTIONS: [article,letterpaper,times,10pt,margin=0.7in]
#+LATEX_HEADER: \usepackage[margin=0.7in]{geometry}
#+AUTHOR: %%AUTHOR%% | %%STDID%%

#+DATE: due: January 28^{rd}, 2022
#+LaTeX_HEADER: \usepackage{lastpage}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bbm}
#+LATEX_HEADER: \pagestyle{fancy}
#+LATEX_HEADER: \chead{} %%AUTHOR%%
#+LATEX_HEADER: \lhead{total pages: \pageref{LastPage}}
#+LATEX_HEADER: \rhead{this is page \thepage}
#+LATEX_HEADER: \lfoot{}
#+LATEX_HEADER: \cfoot{ECON 497 Winter 2022}
#+LATEX_HEADER: \rfoot{}
#+LATEX: \renewcommand{\footrulewidth}{0.4pt}

#+LATEX: \linespread{1.5}

* Q1
Write the model in matrix form[fn::note that I am using the more standard $(\hat\beta_0,\hat\beta_1)^T$ instead of $(\hat\beta_1,\hat\beta_2)^T$ ]:

\[y = \left(y_1, \ldots, y_n \right) = X\beta + \epsilon = \begin{pmatrix} 1      & x_1 \\ \ldots & \ldots \\ 1 & x_n \end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix} + \epsilon\]

** Q1a
We show that the OLS estimator $\hat\beta = (X^TX)^{-1}X^T y$
matches the SLR expresisons:

First, \[X^TX = \begin{pmatrix} \sum_1^n 1 & \sum x_i \\ \sum x_i & \sum x_i^2 \end{pmatrix} = n \begin{pmatrix} 1 & \bar{x} \\ \bar{x} & \bar{x^2} \end{pmatrix} \]

And since $\begin{pmatrix} a & b\\b & c\end{pmatrix}^{-1} = \Delta^{-1}\begin{pmatrix} c & -b \\ -b & a \end{pmatrix}$ with the determinant $\Delta := ac - b^2$, we have[fn::We use a standard convention of denoting the realized sample value of a statistic $T$ by its lowecase $t$, so e.g. $n\bar{x} := \sum{x_i}, \ n\bar{x_i^2} := \sum x_i^2$]

#+NAME: XTX
\begin{equation}
(X^T X)^{-1} = \frac{1}{n(\bar{x_i} - \bar{x}^2)} \begin{pmatrix} \bar{x^2} & -\bar{x} \\ -\bar{x} & 1\end{pmatrix}
\end{equation}

Further, \[X^T y = \begin{pmatrix}n\bar{y}\\ \sum x_i y_i\end{pmatrix}\]
So that \[\hat{\beta} = \begin{pmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{pmatrix} = \Delta^{-1} \begin{pmatrix}n\bar{y}\bar{x^2} - \bar{x}\sum{x_i y_i} \\ \sum x_i y_i - n\bar{x}\bar{y}\end{pmatrix} = \frac{1}{n(\bar{x^2}-\bar{x}^2)}\begin{pmatrix}n\bar{y}\bar{x^2} - \bar{x}\sum{x_i y_i} \\ \sum x_i y_i - n\bar{x}\bar{y}\end{pmatrix} \]

Simple rearrangement will yield for the denominator
#+NAME: beta-denominator
\begin{equation}
\sum(x_i-\bar{x})^2 = \sum x_i^2 - 2\bar{x}\sum{x_i} + n\bar{x}^2 = n(\bar{x^2} - \bar{x}^2)
\end{equation}

Furthermore, by centralizing $u_i := x_i - \bar{x},\ v_i := y_i - \bar{y}$ so that $\bar{u} = \bar{v} = 0$, we have \[\sum u_i v_i = \sum(x_i - \bar{x})(y_i - \bar{y}) = \sum u_i (y_i - \bar{y}) = \sum u_i y_i - n\bar{u}\bar{y} = \sum x_i y_i - n\bar{x}\bar{y} \]
Together, the two expressions above mean we have derived $\hat{\beta_1}$ in the desired form: \[\hat{\beta_1} = \frac{\sum x_i y_i - n\bar{x}\bar{y}}{n(\bar{x^2} - \bar{x}^2)} = \frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}\]
So that \[\hat{\beta_0} = \frac{n\bar{y}\bar{x^2} - \bar{x}\sum{x_i y_i}}{n(\bar{x^2} - \bar{x}^2)} = -\bar{x} \frac{\sum x_i y_i - n \bar{x}\bar{y}}{n(\bar{x^2}-\bar{x}^2)} + \frac{n\bar{y}\bar{x^2}-n\bar{x}^2\bar{y}}{n(\bar{x^2} - \bar{x}^2)} = \bar{y} - \hat{\beta_1}\bar{x}\]

** Q1b
Equations [[XTX]] and [[beta-denominator]] together yield immediately
\[\frac{\mathbbm{V}[\hat{\beta_0}]}{\sigma^2} = \frac{\bar{x^2}}{\sum (x_i - \bar{x})^2}\] and
\[\frac{\mathbbm{V}[\hat{\beta_1}]}{\sigma^2} = \frac{1}{\sum(x_i-\bar{x})^2}
\]

* Q2
Model: \[y = X\beta + \epsilon = X \begin{pmatrix}\beta_1\\ \beta_2 \\ \beta_3 \\ \beta_4\end{pmatrix} + \epsilon\]
Null hypothesis: $H_0: \beta_2 = \beta_2$, alterntive $H_a: \beta_2 \neq 0$ or $\beta_3 \neq 0$.
** Q2a
In matrix form, $H_0$ is written as \[\begin{pmatrix}0 & 1 & 0& 0\\ 0 & 0 & 1 & 0 \end{pmatrix} \begin{pmatrix}\beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \end{pmatrix} = \begin{pmatrix}0\\ 0\end{pmatrix}\]
** Q2b

** Q2c
In matrix form, $H_0$ is written as \[\begin{pmatrix}0 & 1 & 0& -1\\ 0 & 0 & 1 & 0 \end{pmatrix} \begin{pmatrix}\beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \end{pmatrix} = \begin{pmatrix}3\\ 0\end{pmatrix}\]
* Q3
\begin{equation*}
\begin{aligned}
\hat{\mu_1} &:= \frac{1}{n-5}\sum_1^n x_i \\
\hat{\mu_2} &:= \alpha \frac{\sum_1^k x_i}{k} + \beta \frac{\sum_{k+1}^n x_i}{n-k}
\end{aligned}
\end{equation*}
with $0 < k < n$, $0 < \alpha,\beta < 1$, $\alpha + \beta = 1$. Here, $\alpha = \frac{1}{3} = 1-\beta$, $k=1$.

** TL;DR
| estimator $\hat{\mu}$ \ properties | $E[\hat{\mu}]$     | $V[\hat{\mu}]/\sigma^2$                                            | unbiased? | BLUE       | p-$\lim$        | consistent? |
|------------------------------------+--------------------+--------------------------------------------------------------------+-----------+------------+-----------------+-------------|
| $\hat{\mu_1}$                      | $\frac{n}{n-5}\mu$ | $\left(\frac{n-5}{n}\right)^2$                                     | *no*      | no; biased | $\delta(x-\mu)$ | yes         |
| $\hat{\mu_2}$                      | $\mu$              | $\frac{\alpha^2}{k} + \frac{\beta^2}{n-k} \geq \frac{\alpha^2}{k}$ | yes       | no         |                 | *no*        |
