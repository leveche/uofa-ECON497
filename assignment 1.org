#+STARTUP: indent
#+OPTIONS: toc:nil num:nil
#+TITLE: ECON 497 Assignment 1
#+LaTeX_CLASS_OPTIONS: [article,letterpaper,times,10pt,margin=0.7in]
#+LATEX_HEADER: \usepackage[margin=0.7in]{geometry}
#+AUTHOR: %%AUTHOR%% | %%STDID%%

#+DATE: due: January 28^{rd}, 2022
#+LaTeX_HEADER: \usepackage{lastpage}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bbm}
#+LATEX_HEADER: \pagestyle{fancy}
#+LATEX_HEADER: \chead{} %%AUTHOR%%
#+LATEX_HEADER: \lhead{total pages: \pageref{LastPage}}
#+LATEX_HEADER: \rhead{this is page \thepage}
#+LATEX_HEADER: \lfoot{}
#+LATEX_HEADER: \cfoot{ECON 497 Winter 2022}
#+LATEX_HEADER: \rfoot{}
#+LATEX: \renewcommand{\footrulewidth}{0.4pt}

#+LATEX: \linespread{1.5}

* Q1
Write the model in matrix form[fn::note that I am using the more standard $(\hat\beta_0,\hat\beta_1)^T$ instead of $(\hat\beta_1,\hat\beta_2)^T$ ]:

\[y = \left(y_1, \ldots, y_n \right) = X\beta + \epsilon = \begin{pmatrix} 1      & x_1 \\ \ldots & \ldots \\ 1 & x_n \end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix} + \epsilon\]

** Q1a
We show that the OLS estimator $\hat\beta = (X^TX)^{-1}X^T y$
matches the SLR expresisons:

First, \[X^TX = \begin{pmatrix} \sum_1^n 1 & \sum x_i \\ \sum x_i & \sum x_i^2 \end{pmatrix} = n \begin{pmatrix} 1 & \bar{x} \\ \bar{x} & \bar{x^2} \end{pmatrix} \]

And since $\begin{pmatrix} a & b\\b & c\end{pmatrix}^{-1} = \Delta^{-1}\begin{pmatrix} c & -b \\ -b & a \end{pmatrix}$ with the determinant $\Delta := ac - b^2$, we have[fn::We use a standard convention of denoting the realized sample value of a statistic $T$ by its lowecase $t$, so e.g. $n\bar{x} := \sum{x_i}, \ n\bar{x_i^2} := \sum x_i^2$]
\begin{equation}\label{XTX}
(X^T X)^{-1} = \frac{1}{n(\bar{x_i} - \bar{x}^2)} \begin{pmatrix} \bar{x^2} & -\bar{x} \\ -\bar{x} & 1\end{pmatrix}
\end{equation}

Further, \[X^T y = \begin{pmatrix}n\bar{y}\\ \sum x_i y_i\end{pmatrix}\]
So that \[\hat{\beta} = \begin{pmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{pmatrix} = \Delta^{-1} \begin{pmatrix}n\bar{y}\bar{x^2} - \bar{x}\sum{x_i y_i} \\ \sum x_i y_i - n\bar{x}\bar{y}\end{pmatrix} = \frac{1}{n(\bar{x^2}-\bar{x}^2)}\begin{pmatrix}n\bar{y}\bar{x^2} - \bar{x}\sum{x_i y_i} \\ \sum x_i y_i - n\bar{x}\bar{y}\end{pmatrix} \]

Simple rearrangement will yield for the denominator
\begin{equation}\label{beta-denominator}
\sum(x_i-\bar{x})^2 = \sum x_i^2 - 2\bar{x}\sum{x_i} + x\bar{x}^2 = n(\bar{x^2} - \bar{x}^2)
\end{equation}

Furthermore, by centralizing $u_i := x_i - \bar{x},\ v_i := y_i - \bar{y}$ so that $\bar{u} = \bar{v} = 0$, we have \[\sum u_i v_i = \sum(x_i - \bar{x})(y_i - \bar{y}) = \sum u_i (y_i - \bar{y}) = \sum u_i y_i - n\bar{u}\bar{y} = \sum x_i y_i - n\bar{x}\bar{y} \]
Together, the two expressions above mean we have derived $\hat{\beta_1}$ in the desired form: \[\hat{\beta_1} = \frac{\sum x_i y_i - n\bar{x}\bar{y}}{n(\bar{x^2} - \bar{x}^2)} = \frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}\]
So that \[\hat{\beta_0} = \frac{n\bar{y}\bar{x^2} - \bar{x}\sum{x_i y_i}}{n(\bar{x^2} - \bar{x}^2)} = -\bar{x} \frac{\sum x_i y_i - n \bar{x}\bar{y}}{n(\bar{x^2}-\bar{x}^2)} + \frac{n\bar{y}\bar{x^2}-n\bar{x}^2\bar{y}}{n(\bar{x^2} - \bar{x}^2)} = \bar{y} - \hat{\beta_1}\bar{x}\]

** Q1b
Equations \ref{XTX} and \ref{beta-denominator} together yield immediately
\[\frac{\mathbbm{V}[\hat{\beta_0}]}{\sigma^2} = frac{\bar{x^2}}{\sum (x_i - \bar{x})^2}\] and
\[\frac{\mathbbm{V}[\hat{\beta_1}]}{\sigma^2} = \frac{1}{\sum(x_i-\bar{x})^2}
\]
